{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41bdee42-c7a7-4f2d-add5-1b739f8ead7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# The Nintendo Entertainment System Music Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d80ae2-babd-4ce5-9037-1f5e5a0f9de6",
   "metadata": {},
   "source": [
    "\"The Nintendo Entertainment Music Database\", which I will be abbreviaiting to NES-MDB from here on out, is a dataset created by Chris Donahue, Huanru Henry Mao, and Julian McAuley. It contains thousands of songs found within Nintendo Entertainment System video games, with each song containing a musical score for four instrument voices and expressive attributes for the dynamics and timbre of each voice. It is this portion of the NES-MDB that sets it apart from other music databases, as unlike others which typically only contain MIDI files, the NES-MDB contains all of the information necessary to render *exact* acoustic performances of the original compositions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3950b28a-7c7b-4dfe-b99a-f2f65d454c1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Intended Use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9aab2a-86d4-47df-a577-e6855067dd42",
   "metadata": {},
   "source": [
    "The purpose of this database is to bring forth a new wave of polyphonic music generation that puts an emphasis on expressive performance attributes. While there are many other datasets that give a look into similar music compilations, it is the expressive attributes available in this dataset, along with the four distinct instrument voices, that give a clear and concise view of how music was composed for the NES. Along with the dataset, a tool was also created by the authors that allows computer generated compositions to be rendered as NES style audio, which can then be used for researchers to try out their music as waveforms capable of being read by the NES. A tool such as this permits even greater depths to which researchers can explore the finer details of automated music generation for the NES."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf662041-b40b-48a4-a573-9fa9e522a349",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The Score Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff85fdf-54ba-48b4-832a-4842c1cca20a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Within the database, there are a total of six different musical formats that allow unique looks into the structure of the NES music library. The main ones focused on in the paper include the blended composition, separated composition, and expressive compositin, which I will explain in more detail below, as they provide the most use for researchers. Along with these, MIDI files for each track are included which deliver a familiar format to anyone who wishes to view the music from the NES. The NES language modeling (NLM) format is also here. It contains a timestamped list of instructions that control the synthesizer state machine inside the NES. The final format in the database is the video game music (VGM) format. This is where all of the prior formats were derived from, as it is the raw sound file that contains the code for the music. Although these last three formats were not discussed in depth in how they can be used in their own areas of study, they undoubtably have a wide range of use when it comes to studying various topics in general music composition and raw video game music files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6141159a-6266-4f0f-afe4-e4c42ee01862",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Blended Score Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bb1d30-3b19-4342-ad47-b558873a13bb",
   "metadata": {},
   "source": [
    "Much of the prior research on monophonic music composition uses the blended score format, making it the standardized benchmark for seqeuntial models. Due to the format not maintaining the unique instrument voices, however, it is not ideal for NES music. Nevertheless, it can still be quite useful here for comparison between alogrithms as a baseline score.\n",
    "\n",
    "The blended score is calculated by the following formula: *P(c) = P(B1) · P(B2 |B1) · ... · P(BT |Bt<T )*\n",
    "* *B* is binary matrix of size *N x T*\n",
    "* *N* is the number of possible note values (88 for the NES)\n",
    "* *T* is the timestamp of the note\n",
    "* *B[n, t] = 1* if a voice is playing a note *n* at timestamp *t*, and *0* otherwise\n",
    "\n",
    "Below is a depiction of the blended score format for the song *Ending Theme* in the NES game *Abadox*.\n",
    "![Blended Score](https://raw.githubusercontent.com/chrisdonahue/nesmdb/master/static/score_blended.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d93a8c4-bd76-417a-8b40-ffd749767551",
   "metadata": {},
   "source": [
    "## Separated Score Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc5f954-c7e2-4e59-98ee-88da25a6a69d",
   "metadata": {},
   "source": [
    "Unlike the blended format, the separated score format encodes a monophonic sequence for every instrument voice available, rather than only allowing one voice. Due to the nature of the hardware restrictions within the NES APU only allowing four unique instrument voices, this format delivers an ideal way to view the sequence in which the voices are played.\n",
    "\n",
    "The separated score is calculated by the following format: *P(c) = ∏(T, t=1) · ∏(V, v=1) · P(Sv,t | Sv,t̂≠t, Sv̂≠v,∀t̂)*\n",
    "* *S* is a matrix of size *V x T*\n",
    "* *V* is the number of instrument voices\n",
    "* *T* is the timestamp of the note\n",
    "* *S[v, t] = n* represents the note *n* played by the voice *v* at timestamp *t*\n",
    "\n",
    "Below is a depiction of the separated score format for the song *Ending Theme* in the NES game *Abadox*.\n",
    "![Seperated Score](https://raw.githubusercontent.com/chrisdonahue/nesmdb/master/static/score_separated.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec62c2b-25ea-4e5b-8cf2-963aab6016c2",
   "metadata": {},
   "source": [
    "## Expressive Score Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b17a32-3dc0-459f-b18c-4f53497d2916",
   "metadata": {},
   "source": [
    "The final format on display is where we get to view the expressive attributes that makes this dataset special. The expressive score format builds off of the separated score format, but adds on the dynamics such as velocity and timbre of each note. It is this format that the creators of the database hope to bring about revolution in polyphonic music generation.\n",
    "\n",
    "The expressive score is calculated by the following format: *P(m) = P(e|c) · P(c)*\n",
    "* *P(e|c)* is the mapping of a composition *c* onto an expressive characteristic *e*\n",
    "* *P(c)* is taken from the separated score\n",
    "\n",
    "Below is a depiction of the expressive score format for the song *Ending Theme* in the NES game *Abadox*.\n",
    "![Expressive Score](https://raw.githubusercontent.com/chrisdonahue/nesmdb/master/static/score_expressive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5231d8-0cdd-4e31-9530-f6dbc8c00de3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Use in Other Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990b3279-666a-4ed5-b7d4-29b684e8efea",
   "metadata": {},
   "source": [
    "One paper that I will highlight that uses this dataset is \"Towards Automatic Instrumentation By Learning to Separate Parts in Symbolic Multitrack Music,\" by Hao-Wen Dong, et al. The goal of this project is to study the feasibility of automatic music generation dynamically assigning instrument voices to different notes within a solo music performance. This project uses the NES-MDB as one of four datasets that they utilize to examine the effectiveness of the models that are proposed within the paper. The paper goes into great detail about how they alter and implement the NES-MDB to fit their needs for their models, but the ultimate outcome here is their models succeed in outperforming various baselines. This paper is a great example of the potential that the NES-MDB has in aiding in the study of a wide variety of music based studies. Even though the goal of this paper is not how the creators of the NES-MDB had imagined their dataset would be used, the flexibility and numerous formats provided within the dataset allowed Hao-Wen Dong and his team to use it to better understand part separation in mulitrack music."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b94ce4b-710c-45b3-a0b4-56a1a2d9f2fd",
   "metadata": {},
   "source": [
    "## Look at the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cea8f2",
   "metadata": {},
   "source": [
    "Due to the unique nature of the NES-MDB, there isn't an easy way to view all of the information available within the dataset in something like an Excel document, for example. If someone wished to look at the expressive score format for *Mega Man 2*'s track *Title*, they would have to execute the code as followes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc5b9342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal discretization rate: 24.0\n",
      "Length of original VGM: 42.58047619047619\n",
      "Piano roll shape: (1021, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('225_MegaMan2_01_02Title.exprsco.pkl', 'rb') as f:\n",
    "  rate, nsamps, exprsco = pickle.load(f)\n",
    "\n",
    "print('Temporal discretization rate: {}'.format(rate)) # Will be 24.0\n",
    "print('Length of original VGM: {}'.format(nsamps / 44100.))\n",
    "print('Piano roll shape: {}'.format(exprsco.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9efb1d",
   "metadata": {},
   "source": [
    "In order to render your generated music through an emulation of the NES synthesizer, the creators of the NES-MDB created their own package titled `nesmdb`. This package also features the functionality to convert between the formats available in the dataset, as well as turning any format into a *.wav* file. Here is what someone would run if they wished to convert the expressive format of *Mega Man 2*'s track *Title* to a WAV file:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "297a7e39",
   "metadata": {},
   "source": [
    "from nesmdb.convert import midi_to_wav\n",
    "with open('225_MegaMan2_01_02Title.exprsco.pkl', 'rb') as f:\n",
    "  mid = f.read()\n",
    "# Quantizes MIDI to 100Hz before rendering\n",
    "# Can set to None to avoid quantization but will take more time/memory\n",
    "wav = midi_to_wav(mid, midi_to_wav_rate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d06ee3a",
   "metadata": {},
   "source": [
    "Perhaps the coolest thing about the `nesmdb` package is its capabality to emulate the NES APU from within python. Donahue and his team needed to build the functionallity in order to extract the information needed from the VGM files so that they could build all of the other formats. There is a large portion of code that went into this, but here is a look at the code used to extract the information about the pulse wave voices from the music."
   ]
  },
  {
   "cell_type": "raw",
   "id": "26b3af28",
   "metadata": {},
   "source": [
    "import nesmdb.apu   \n",
    "   \n",
    "# Fill in pulse score\n",
    "for i, ch in zip([0, 1], ['p1', 'p2']):\n",
    "  if ch_to_length_counter[ch] > 0:\n",
    "    vo = ch_to_vo[ch] if ch_to_cv[ch] else ch_to_env_decay_level[ch]\n",
    "    if vo > 0 and ch_to_timer[ch] >= 8 and ch_to_stimer[ch] < 0x800:\n",
    "      rawsco[samp, i, 0] = (ch_to_timer[ch] & 0b11100000000) >> 8\n",
    "      rawsco[samp, i, 1] = (ch_to_timer[ch] & 0b00011111111)\n",
    "      rawsco[samp, i, 2] = vo\n",
    "  rawsco[samp, i, 3] = ch_to_du[ch]\n",
    "  ch_to_retrigger[ch] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98cea50-62bf-4b6c-8aaa-ca17101e8191",
   "metadata": {},
   "source": [
    "## Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb111b0c-a6a8-4bb0-a8fb-40fb04e5c9f6",
   "metadata": {},
   "source": [
    "Chris Donahue, Huanru Henry Mao, and Julian McAuley, “The NES Music Database: A Multi-Instrumental Dataset with Expressive Performance Attributes,” 19th International Society for Music Information Retrieval Conference, Paris, France, 2018.\n",
    "\n",
    "Hao-Wen Dong, Chris Donahue, Taylor Berg-Kirkpatrick, Julian McAuley, \"Towards Automatic Instrumentation By Learning to Separate Parts in Symbolic Multitrack Music,\" 22nd International Society for Information Retrieval Conference, Online, 2021"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
